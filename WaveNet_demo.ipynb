{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook gives a quick overview of this WaveNet implementation, i.e. creating the model and the data set, training the model and generating samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 14:16:06.176581: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset #WavenetDataset() uses librosa\n",
    "from wavenet_training import *\n",
    "from model_logging import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Model\n",
    "This is an implementation of WaveNet as it was described in the original paper (https://arxiv.org/abs/1609.03499). Each layer looks like this:\n",
    "\n",
    "```\n",
    "            |----------------------------------------|      *residual*\n",
    "            |                                        |\n",
    "            |    |-- conv -- tanh --|                |\n",
    " -> dilate -|----|                  * ----|-- 1x1 -- + -->  *input*\n",
    "                 |-- conv -- sigm --|     |\n",
    "                                         1x1\n",
    "                                          |\n",
    " ---------------------------------------> + ------------->  *skip*\n",
    "```\n",
    "\n",
    "Each layer dilates the input by a factor of two. After each block the dilation is reset and start from one. You can define the number of layers in each block (``layers``) and the number of blocks (``blocks``). The blocks are followed by two 1x1 convolutions and a softmax output function.\n",
    "Because of the dilation operation, the independent output for multiple successive samples can be calculated efficiently. With ``output_length``, you can define the number these outputs. Empirically, it seems that a large number of skip channels is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# initialize cuda option\n",
    "dtype = torch.FloatTensor # data type\n",
    "ltype = torch.LongTensor # label type\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    ltype = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * * * * * * * * * * *\n",
      "in block 0\n",
      "in layer 0\n",
      "receptive field:  2\n",
      "in layer 1\n",
      "receptive field:  4\n",
      "in layer 2\n",
      "receptive field:  8\n",
      "in layer 3\n",
      "receptive field:  16\n",
      "in layer 4\n",
      "receptive field:  32\n",
      "in layer 5\n",
      "receptive field:  64\n",
      "in layer 6\n",
      "receptive field:  128\n",
      "in layer 7\n",
      "receptive field:  256\n",
      "in layer 8\n",
      "receptive field:  512\n",
      "in layer 9\n",
      "receptive field:  1024\n",
      "* * * * * * * * * * * *\n",
      "in block 1\n",
      "in layer 0\n",
      "receptive field:  1025\n",
      "in layer 1\n",
      "receptive field:  1027\n",
      "in layer 2\n",
      "receptive field:  1031\n",
      "in layer 3\n",
      "receptive field:  1039\n",
      "in layer 4\n",
      "receptive field:  1055\n",
      "in layer 5\n",
      "receptive field:  1087\n",
      "in layer 6\n",
      "receptive field:  1151\n",
      "in layer 7\n",
      "receptive field:  1279\n",
      "in layer 8\n",
      "receptive field:  1535\n",
      "in layer 9\n",
      "receptive field:  2047\n",
      "* * * * * * * * * * * *\n",
      "in block 2\n",
      "in layer 0\n",
      "receptive field:  2048\n",
      "in layer 1\n",
      "receptive field:  2050\n",
      "in layer 2\n",
      "receptive field:  2054\n",
      "in layer 3\n",
      "receptive field:  2062\n",
      "in layer 4\n",
      "receptive field:  2078\n",
      "in layer 5\n",
      "receptive field:  2110\n",
      "in layer 6\n",
      "receptive field:  2174\n",
      "in layer 7\n",
      "receptive field:  2302\n",
      "in layer 8\n",
      "receptive field:  2558\n",
      "in layer 9\n",
      "receptive field:  3070\n",
      "model:  WaveNetModel(\n",
      "  (filter_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (gate_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (residual_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (skip_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (start_conv): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "receptive field:  3070\n",
      "parameter count:  1834592\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetModel(layers=10,\n",
    "                     blocks=3,\n",
    "                     dilation_channels=32,\n",
    "                     residual_channels=32,\n",
    "                     skip_channels=1024,\n",
    "                     end_channels=512, \n",
    "                     output_length=16, # is this how many predictions we make?\n",
    "                     dtype=dtype, \n",
    "                     bias=True)\n",
    "# model = load_latest_model_from('snapshots', use_cuda=use_cuda)\n",
    "\n",
    "print('model: ', model)\n",
    "print('receptive field: ', model.receptive_field)\n",
    "print('parameter count: ', model.parameter_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "To create the data set, you have to specify a path to a data set file. If this file already exists it will be used, if not it will be generated. If you want to generate the data set file (a ``.npz`` file), you have to specify the directory (``file_location``) in which all the audio files you want to use are located. The attribute ``target_length`` specifies the number of successive samples are used as a target and corresponds to the output length of the model. The ``item_length`` defines the number of samples in each item of the dataset and should always be ``model.receptive_field + model.output_length - 1``.\n",
    "\n",
    "```\n",
    "          |----receptive_field----|\n",
    "                                |--output_length--|\n",
    "example:  | | | | | | | | | | | | | | | | | | | | |\n",
    "target:                           | | | | | | | | | |  \n",
    "```\n",
    "To create a test set, you should define a ``test_stride``. Then each ``test_stride``th item will be assigned to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot input\n",
      "the dataset has 593481 items\n",
      "torch.Size([256, 3085])\n"
     ]
    }
   ],
   "source": [
    "data = WavenetDataset(dataset_file='train_samples/temp_dataset.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      file_location='train_samples/bach_chaconne',\n",
    "                      test_stride=500)\n",
    "print('the dataset has ' + str(len(data)) + ' items')\n",
    "print(f'{data[100][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Logging\n",
    "This implementation supports logging with TensorBoard (you need to have TensorFlow installed). You can even generate audio samples from the current snapshot of the model during training. This will happen in a background thread on the cpu, so it will not interfere with the actual training but will be rather slow. If you don't have TensorFlow, you can use the standard logger that will print out to the console.\n",
    "The trainer uses Adam as default optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.10.0\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "def generate_and_log_samples(step):\n",
    "    sample_length=32000\n",
    "    gen_model = load_latest_model_from('snapshots', use_cuda=False)\n",
    "    print(\"start generating...\")\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[0.5])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    logger.audio_summary('temperature_0.5', tf_samples, step, sr=16000)\n",
    "\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[1.])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    logger.audio_summary('temperature_1.0', tf_samples, step, sr=16000)\n",
    "    print(\"audio clips generated\")\n",
    "\n",
    "\n",
    "logger = TensorboardLogger(log_interval=200,\n",
    "                           validation_interval=400,\n",
    "                           generate_interval=1000,\n",
    "                           generate_function=generate_and_log_samples,\n",
    "                           log_dir=\"logs/temp_model\")\n",
    "\n",
    "# logger = Logger(log_interval=200,\n",
    "#                 validation_interval=400,\n",
    "#                 generate_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch 0\n",
      "model() called\n",
      "forward() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "after x is defined\n",
      "model() called\n",
      "forward() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "after x is defined\n",
      "model() called\n",
      "forward() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "after x is defined\n",
      "model() called\n",
      "forward() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "after x is defined\n",
      "model() called\n",
      "forward() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "after x is defined\n",
      "model() called\n",
      "forward() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "wavenet_dilate() called\n",
      "dilate() called\n",
      "dilate() called\n",
      "after x is defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m WavenetTrainer(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m      2\u001b[0m                          dataset\u001b[39m=\u001b[39mdata,\n\u001b[1;32m      3\u001b[0m                          lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                          dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m      9\u001b[0m                          ltype\u001b[39m=\u001b[39mltype)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstart training...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m               epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_training.py:73\u001b[0m, in \u001b[0;36mWavenetTrainer.train\u001b[0;34m(self, batch_size, epochs, continue_training_at_step)\u001b[0m\n\u001b[1;32m     71\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output\u001b[39m.\u001b[39msqueeze(), target\u001b[39m.\u001b[39msqueeze())\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 73\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     74\u001b[0m \u001b[39m# print(type(loss))\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# loss = loss.data[0]\u001b[39;00m\n\u001b[1;32m     76\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/untitled/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/untitled/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/untitled/lib/python3.10/site-packages/torch/autograd/function.py:277\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBackwardCFunction\u001b[39;00m(_C\u001b[39m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    278\u001b[0m         \u001b[39m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[39m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         backward_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mbackward  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m         vjp_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_cls\u001b[39m.\u001b[39mvjp  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = WavenetTrainer(model=model,\n",
    "                         dataset=data,\n",
    "                         lr=0.001,\n",
    "                         snapshot_path='snapshots',\n",
    "                         snapshot_name='chaconne_model',\n",
    "                         snapshot_interval=1000,\n",
    "                         logger=logger,\n",
    "                         dtype=dtype,\n",
    "                         ltype=ltype)\n",
    "\n",
    "print('start training...')\n",
    "trainer.train(batch_size=1,\n",
    "              epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating\n",
    "This model has the Fast Wavenet Generation Algorithm (https://arxiv.org/abs/1611.09482) implemented. This might run faster on the cpu. You can give some starting data (of at least the length of receptive field) or let the model generate from zero. In my experience, a temperature between 0.5 and 1.0 yields the best results, but this may depend on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[32, 1]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprog_callback\u001b[39m(step, total_steps):\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mstr\u001b[39m(\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m step \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m total_steps) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m% g\u001b[39;00m\u001b[39menerated\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m generated \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate_fast(num_samples\u001b[39m=\u001b[39;49m\u001b[39m160000\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m                                  first_samples\u001b[39m=\u001b[39;49mstart_data,\n\u001b[1;32m      9\u001b[0m                                  progress_callback\u001b[39m=\u001b[39;49mprog_callback,\n\u001b[1;32m     10\u001b[0m                                  progress_interval\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m                                  temperature\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m                                  regularize\u001b[39m=\u001b[39;49m\u001b[39m0.\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_model.py:267\u001b[0m, in \u001b[0;36mWaveNetModel.generate_fast\u001b[0;34m(self, num_samples, first_samples, temperature, regularize, progress_callback, progress_interval)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m# fill queues with given samples\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_given_samples \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwavenet(\u001b[39minput\u001b[39;49m,\n\u001b[1;32m    268\u001b[0m                      dilation_func\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqueue_dilate)\n\u001b[1;32m    269\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mzero_()\n\u001b[1;32m    270\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mscatter_(\u001b[39m1\u001b[39m, first_samples[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), \u001b[39m1.\u001b[39m)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_model.py:148\u001b[0m, in \u001b[0;36mWaveNetModel.wavenet\u001b[0;34m(self, input, dilation_func)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m     \u001b[39m#            |----------------------------------------|     *residual*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39m#                                          |\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# ---------------------------------------> + ------------->\t*skip*\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     (dilation, init_dilation) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilations[i]\n\u001b[0;32m--> 148\u001b[0m     residual \u001b[39m=\u001b[39m dilation_func(x, dilation, init_dilation, i)\n\u001b[1;32m    150\u001b[0m     \u001b[39m# dilated convolution\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[39mfilter\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_convs[i](residual)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_model.py:184\u001b[0m, in \u001b[0;36mWaveNetModel.queue_dilate\u001b[0;34m(self, input, dilation, init_dilation, i)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_dilate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, dilation, init_dilation, i):\n\u001b[1;32m    183\u001b[0m     queue \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilated_queues[i]\n\u001b[0;32m--> 184\u001b[0m     queue\u001b[39m.\u001b[39;49menqueue(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mdata[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    185\u001b[0m     x \u001b[39m=\u001b[39m queue\u001b[39m.\u001b[39mdequeue(num_deq\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,\n\u001b[1;32m    186\u001b[0m                       dilation\u001b[39m=\u001b[39mdilation)\n\u001b[1;32m    187\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_modules.py:57\u001b[0m, in \u001b[0;36mDilatedQueue.enqueue\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menqueue\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[:, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_pos] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_pos \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_pos \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[32, 1]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "start_data = data[250000][0] # use start data from the data set\n",
    "start_data = torch.max(start_data, 0)[1] # convert one hot vectors to integers\n",
    "\n",
    "def prog_callback(step, total_steps):\n",
    "    print(str(100 * step // total_steps) + \"% generated\")\n",
    "\n",
    "generated = model.generate_fast(num_samples=160000,\n",
    "                                 first_samples=start_data,\n",
    "                                 progress_callback=prog_callback,\n",
    "                                 progress_interval=1000,\n",
    "                                 temperature=1.0,\n",
    "                                 regularize=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(generated, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "untitled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
