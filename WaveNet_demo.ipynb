{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook gives a quick overview of this WaveNet implementation, i.e. creating the model and the data set, training the model and generating samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install tensorflow\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip uninstall torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ethan\\anaconda3\\envs\\pythonProject\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# checking which conda environment this kernel is using\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset #WavenetDataset() uses librosa\n",
    "from wavenet_training import *\n",
    "from model_logging import *\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "This is an implementation of WaveNet as it was described in the original paper (https://arxiv.org/abs/1609.03499). Each layer looks like this:\n",
    "\n",
    "```\n",
    "            |----------------------------------------|      *residual*\n",
    "            |                                        |\n",
    "            |    |-- conv -- tanh --|                |\n",
    " -> dilate -|----|                  * ----|-- 1x1 -- + -->  *input*\n",
    "                 |-- conv -- sigm --|     |\n",
    "                                         1x1\n",
    "                                          |\n",
    " ---------------------------------------> + ------------->  *skip*\n",
    "```\n",
    "\n",
    "Each layer dilates the input by a factor of two. After each block the dilation is reset and start from one. You can define the number of layers in each block (``layers``) and the number of blocks (``blocks``). The blocks are followed by two 1x1 convolutions and a softmax output function.\n",
    "Because of the dilation operation, the independent output for multiple successive samples can be calculated efficiently. With ``output_length``, you can define the number these outputs. Empirically, it seems that a large number of skip channels is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu\n"
     ]
    }
   ],
   "source": [
    "# initialize cuda option\n",
    "dtype = torch.FloatTensor # data type\n",
    "ltype = torch.LongTensor # label type\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    # dtype=torch.tensor(data=None,device=\"cuda\")\n",
    "    ltype = torch.cuda.LongTensor\n",
    "else:\n",
    "    print('not using gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  WaveNetModel(\n",
      "  (filter_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 24, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (gate_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 24, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (residual_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(24, 32, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (skip_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(24, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (start_conv): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "receptive field:  3070\n",
      "parameter count:  1549952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ethan\\Documents\\MATH 479 (Math Modelling)\\WaveNet-Implementation\\wavenet_modules.py:54: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  self.data = Variable(dtype(num_channels, max_length).zero_())\n"
     ]
    }
   ],
   "source": [
    "model = WaveNetModel(layers=10,\n",
    "                     blocks=3,\n",
    "                     dilation_channels=24,\n",
    "                     residual_channels=32,\n",
    "                     skip_channels=1024,\n",
    "                     end_channels=512, \n",
    "                     output_length=16, # is this how many predictions we make?\n",
    "                     dtype=dtype, \n",
    "                     bias=True)\n",
    "model.to(\"cuda\")\n",
    "# model = load_latest_model_from('snapshots', use_cuda=use_cuda)\n",
    "\n",
    "print('model: ', model)\n",
    "print('receptive field: ', model.receptive_field)\n",
    "print('parameter count: ', model.parameter_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "To create the data set, you have to specify a path to a data set file. If this file already exists it will be used, if not it will be generated. If you want to generate the data set file (a ``.npz`` file), you have to specify the directory (``file_location``) in which all the audio files you want to use are located. The attribute ``target_length`` specifies the number of successive samples are used as a target and corresponds to the output length of the model. The ``item_length`` defines the number of samples in each item of the dataset and should always be ``model.receptive_field + model.output_length - 1``.\n",
    "\n",
    "```\n",
    "          |----receptive_field----|\n",
    "                                |--output_length--|\n",
    "example:  | | | | | | | | | | | | | | | | | | | | |\n",
    "target:                           | | | | | | | | | |  \n",
    "```\n",
    "To create a test set, you should define a ``test_stride``. Then each ``test_stride``th item will be assigned to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot input\n",
      "the dataset has 61335 items\n",
      "torch.Size([256, 3085])\n"
     ]
    }
   ],
   "source": [
    "data = WavenetDataset(dataset_file='train_samples/dataset.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      file_location='train_samples',\n",
    "                      test_stride=64)\n",
    "print('the dataset has ' + str(len(data)) + ' items')\n",
    "print(f'{data[100][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Logging\n",
    "This implementation supports logging with TensorBoard (you need to have TensorFlow installed). You can even generate audio samples from the current snapshot of the model during training. This will happen in a background thread on the cpu, so it will not interfere with the actual training but will be rather slow. If you don't have TensorFlow, you can use the standard logger that will print out to the console.\n",
    "The trainer uses Adam as default optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "def generate_and_log_samples(step):\n",
    "    sample_length=32000\n",
    "    gen_model = load_latest_model_from('snapshots', use_cuda=False)\n",
    "    print(\"start generating...\")\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[0.5])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    logger.audio_summary('temperature_0.5', tf_samples, step, sr=16000)\n",
    "\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[1.])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    logger.audio_summary('temperature_1.0', tf_samples, step, sr=16000)\n",
    "    print(\"audio clips generated\")\n",
    "\n",
    "\n",
    "logger = TensorboardLogger(log_interval=200,\n",
    "                           validation_interval=400,\n",
    "                           generate_interval=1000,\n",
    "                           generate_function=generate_and_log_samples,\n",
    "                           log_dir=\"logs/temp_model\")\n",
    "\n",
    "# logger = Logger(log_interval=200,\n",
    "#                 validation_interval=400,\n",
    "#                 generate_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch 0\n",
      "one training step does take approximately 0.4732839584350586 seconds)\n",
      "WARNING:tensorflow:From C:\\Users\\Ethan\\Documents\\MATH 479 (Math Modelling)\\WaveNet-Implementation\\model_logging.py:100: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_ResourceSummaryWriter' object has no attribute 'add_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m WavenetTrainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      2\u001b[0m                          dataset\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m      3\u001b[0m                          lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m                          dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m      9\u001b[0m                          ltype\u001b[38;5;241m=\u001b[39mltype)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart training...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MATH 479 (Math Modelling)\\WaveNet-Implementation\\wavenet_training.py:98\u001b[0m, in \u001b[0;36mWavenetTrainer.train\u001b[1;34m(self, batch_size, epochs, continue_training_at_step)\u001b[0m\n\u001b[0;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnapshot_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnapshot_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m time_string)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# print(\"step is:\")\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\MATH 479 (Math Modelling)\\WaveNet-Implementation\\model_logging.py:32\u001b[0m, in \u001b[0;36mLogger.log\u001b[1;34m(self, current_step, current_loss)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulated_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current_loss\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulated_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\MATH 479 (Math Modelling)\\WaveNet-Implementation\\model_logging.py:79\u001b[0m, in \u001b[0;36mTensorboardLogger.log_loss\u001b[1;34m(self, current_step)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlog_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_step):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# loss\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulated_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscalar_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# parameter histograms\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tag, value, \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "File \u001b[1;32m~\\Documents\\MATH 479 (Math Modelling)\\WaveNet-Implementation\\model_logging.py:102\u001b[0m, in \u001b[0;36mTensorboardLogger.scalar_summary\u001b[1;34m(self, tag, value, step)\u001b[0m\n\u001b[0;32m    100\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdisable_eager_execution() \u001b[38;5;66;03m# need to enable this so that old versions of graphs & charts will work\u001b[39;00m\n\u001b[0;32m    101\u001b[0m summary \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSummary(value\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mSummary\u001b[38;5;241m.\u001b[39mValue(tag\u001b[38;5;241m=\u001b[39mtag, simple_value\u001b[38;5;241m=\u001b[39mvalue)])\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_summary\u001b[49m(summary, step)\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_ResourceSummaryWriter' object has no attribute 'add_summary'"
     ]
    }
   ],
   "source": [
    "trainer = WavenetTrainer(model=model,\n",
    "                         dataset=data,\n",
    "                         lr=0.001,\n",
    "                         snapshot_path='snapshots',\n",
    "                         snapshot_name='chaconne_model',\n",
    "                         snapshot_interval=1000,\n",
    "                         logger=logger,\n",
    "                         dtype=dtype,\n",
    "                         ltype=ltype)\n",
    "\n",
    "print('start training...')\n",
    "trainer.train(batch_size=4,\n",
    "              epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating\n",
    "This model has the Fast Wavenet Generation Algorithm (https://arxiv.org/abs/1611.09482) implemented. This might run faster on the cpu. You can give some starting data (of at least the length of receptive field) or let the model generate from zero. In my experience, a temperature between 0.5 and 1.0 yields the best results, but this may depend on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[32, 1]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprog_callback\u001b[39m(step, total_steps):\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mstr\u001b[39m(\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m step \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m total_steps) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m% g\u001b[39;00m\u001b[39menerated\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m generated \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate_fast(num_samples\u001b[39m=\u001b[39;49m\u001b[39m160000\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m                                  first_samples\u001b[39m=\u001b[39;49mstart_data,\n\u001b[1;32m      9\u001b[0m                                  progress_callback\u001b[39m=\u001b[39;49mprog_callback,\n\u001b[1;32m     10\u001b[0m                                  progress_interval\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m                                  temperature\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m                                  regularize\u001b[39m=\u001b[39;49m\u001b[39m0.\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_model.py:267\u001b[0m, in \u001b[0;36mWaveNetModel.generate_fast\u001b[0;34m(self, num_samples, first_samples, temperature, regularize, progress_callback, progress_interval)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m# fill queues with given samples\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_given_samples \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwavenet(\u001b[39minput\u001b[39;49m,\n\u001b[1;32m    268\u001b[0m                      dilation_func\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqueue_dilate)\n\u001b[1;32m    269\u001b[0m     \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mzero_()\n\u001b[1;32m    270\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mscatter_(\u001b[39m1\u001b[39m, first_samples[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m:i \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), \u001b[39m1.\u001b[39m)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_model.py:148\u001b[0m, in \u001b[0;36mWaveNetModel.wavenet\u001b[0;34m(self, input, dilation_func)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m     \u001b[39m#            |----------------------------------------|     *residual*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39m#                                          |\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39m# ---------------------------------------> + ------------->\t*skip*\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     (dilation, init_dilation) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilations[i]\n\u001b[0;32m--> 148\u001b[0m     residual \u001b[39m=\u001b[39m dilation_func(x, dilation, init_dilation, i)\n\u001b[1;32m    150\u001b[0m     \u001b[39m# dilated convolution\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[39mfilter\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_convs[i](residual)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_model.py:184\u001b[0m, in \u001b[0;36mWaveNetModel.queue_dilate\u001b[0;34m(self, input, dilation, init_dilation, i)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqueue_dilate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, dilation, init_dilation, i):\n\u001b[1;32m    183\u001b[0m     queue \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilated_queues[i]\n\u001b[0;32m--> 184\u001b[0m     queue\u001b[39m.\u001b[39;49menqueue(\u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mdata[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    185\u001b[0m     x \u001b[39m=\u001b[39m queue\u001b[39m.\u001b[39mdequeue(num_deq\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size,\n\u001b[1;32m    186\u001b[0m                       dilation\u001b[39m=\u001b[39mdilation)\n\u001b[1;32m    187\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/needed-files/wavenet_modules.py:57\u001b[0m, in \u001b[0;36mDilatedQueue.enqueue\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menqueue\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[:, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_pos] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_pos \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_pos \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[32, 1]}, size=[32]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (2)"
     ]
    }
   ],
   "source": [
    "start_data = data[250000][0] # use start data from the data set\n",
    "start_data = torch.max(start_data, 0)[1] # convert one hot vectors to integers\n",
    "\n",
    "def prog_callback(step, total_steps):\n",
    "    print(str(100 * step // total_steps) + \"% generated\")\n",
    "\n",
    "generated = model.generate_fast(num_samples=160000,\n",
    "                                 first_samples=start_data,\n",
    "                                 progress_callback=prog_callback,\n",
    "                                 progress_interval=1000,\n",
    "                                 temperature=1.0,\n",
    "                                 regularize=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(generated, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
